xgb_Wd_sftmx_md10_x7x3_5e3_4
{'lr': 0.0001, 'batch_size': 20000, 'BS': 100000, 'N_epochs': 200, 'seed': 0}
>> Training...
  0%|          | 0/200 [00:00<?, ?it/s]/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  theta = torch.tensor(theta_init)
/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(out)
  0%|          | 0/200 [00:11<?, ?it/s, test_loss=23.2, train_loss=19]  0%|          | 1/200 [00:11<37:00, 11.16s/it, test_loss=23.2, train_loss=19]  0%|          | 1/200 [00:22<37:00, 11.16s/it, test_loss=22.2, train_loss=18.2]  1%|          | 2/200 [00:22<36:35, 11.09s/it, test_loss=22.2, train_loss=18.2]  1%|          | 2/200 [00:33<36:35, 11.09s/it, test_loss=21.3, train_loss=17.4]  2%|▏         | 3/200 [00:33<36:05, 10.99s/it, test_loss=21.3, train_loss=17.4]  2%|▏         | 3/200 [00:43<36:05, 10.99s/it, test_loss=20.5, train_loss=16.6]  2%|▏         | 4/200 [00:43<35:36, 10.90s/it, test_loss=20.5, train_loss=16.6]  2%|▏         | 4/200 [00:54<35:36, 10.90s/it, test_loss=19.7, train_loss=16]    2%|▎         | 5/200 [00:54<35:17, 10.86s/it, test_loss=19.7, train_loss=16]  2%|▎         | 5/200 [01:05<35:17, 10.86s/it, test_loss=18.8, train_loss=15.3]  3%|▎         | 6/200 [01:05<34:57, 10.81s/it, test_loss=18.8, train_loss=15.3]  3%|▎         | 6/200 [01:16<34:57, 10.81s/it, test_loss=17.9, train_loss=14.5]  4%|▎         | 7/200 [01:16<34:45, 10.80s/it, test_loss=17.9, train_loss=14.5]  4%|▎         | 7/200 [01:27<34:45, 10.80s/it, test_loss=16.6, train_loss=13.5]  4%|▍         | 8/200 [01:27<35:00, 10.94s/it, test_loss=16.6, train_loss=13.5]  4%|▍         | 8/200 [01:38<35:00, 10.94s/it, test_loss=15.5, train_loss=12.6]  4%|▍         | 9/200 [01:38<34:52, 10.96s/it, test_loss=15.5, train_loss=12.6]  4%|▍         | 9/200 [01:49<34:52, 10.96s/it, test_loss=14.6, train_loss=11.8]  5%|▌         | 10/200 [01:49<34:50, 11.00s/it, test_loss=14.6, train_loss=11.8]  5%|▌         | 10/200 [02:00<34:50, 11.00s/it, test_loss=13.3, train_loss=10.9]  6%|▌         | 11/200 [02:00<35:03, 11.13s/it, test_loss=13.3, train_loss=10.9]  6%|▌         | 11/200 [02:12<35:03, 11.13s/it, test_loss=11.7, train_loss=9.53]  6%|▌         | 12/200 [02:12<35:06, 11.20s/it, test_loss=11.7, train_loss=9.53]  6%|▌         | 12/200 [02:23<35:06, 11.20s/it, test_loss=8.83, train_loss=7.52]  6%|▋         | 13/200 [02:23<34:50, 11.18s/it, test_loss=8.83, train_loss=7.52]  6%|▋         | 13/200 [02:34<34:50, 11.18s/it, test_loss=7.03, train_loss=5.77]  7%|▋         | 14/200 [02:34<34:23, 11.09s/it, test_loss=7.03, train_loss=5.77]  7%|▋         | 14/200 [02:45<34:23, 11.09s/it, test_loss=5.12, train_loss=3.84]  8%|▊         | 15/200 [02:45<33:54, 11.00s/it, test_loss=5.12, train_loss=3.84]  8%|▊         | 15/200 [02:56<33:54, 11.00s/it, test_loss=3.84, train_loss=3.23]  8%|▊         | 16/200 [02:56<33:47, 11.02s/it, test_loss=3.84, train_loss=3.23]  8%|▊         | 16/200 [03:06<33:47, 11.02s/it, test_loss=3.84, train_loss=3.19]  8%|▊         | 17/200 [03:06<33:23, 10.95s/it, test_loss=3.84, train_loss=3.19]  8%|▊         | 17/200 [03:17<33:23, 10.95s/it, test_loss=3.86, train_loss=3.2]   9%|▉         | 18/200 [03:17<33:19, 10.99s/it, test_loss=3.86, train_loss=3.2]  9%|▉         | 18/200 [03:28<33:19, 10.99s/it, test_loss=3.58, train_loss=3]   10%|▉         | 19/200 [03:28<32:56, 10.92s/it, test_loss=3.58, train_loss=3] 10%|▉         | 19/200 [03:39<32:56, 10.92s/it, test_loss=3.81, train_loss=3.19] 10%|█         | 20/200 [03:39<32:46, 10.92s/it, test_loss=3.81, train_loss=3.19] 10%|█         | 20/200 [03:53<32:46, 10.92s/it, test_loss=3.76, train_loss=3.05] 10%|█         | 21/200 [03:53<35:08, 11.78s/it, test_loss=3.76, train_loss=3.05] 10%|█         | 21/200 [04:06<35:08, 11.78s/it, test_loss=3.65, train_loss=2.88] 11%|█         | 22/200 [04:06<36:30, 12.30s/it, test_loss=3.65, train_loss=2.88] 11%|█         | 22/200 [04:20<36:30, 12.30s/it, test_loss=3.72, train_loss=3.03] 12%|█▏        | 23/200 [04:20<37:20, 12.66s/it, test_loss=3.72, train_loss=3.03] 12%|█▏        | 23/200 [04:34<37:20, 12.66s/it, test_loss=3.57, train_loss=2.82] 12%|█▏        | 24/200 [04:34<38:03, 12.98s/it, test_loss=3.57, train_loss=2.82] 12%|█▏        | 24/200 [04:47<38:03, 12.98s/it, test_loss=3.52, train_loss=2.82] 12%|█▎        | 25/200 [04:47<38:13, 13.11s/it, test_loss=3.52, train_loss=2.82] 12%|█▎        | 25/200 [05:01<38:13, 13.11s/it, test_loss=3.74, train_loss=3.01] 13%|█▎        | 26/200 [05:01<38:23, 13.24s/it, test_loss=3.74, train_loss=3.01] 13%|█▎        | 26/200 [05:14<38:23, 13.24s/it, test_loss=3.63, train_loss=2.93] 14%|█▎        | 27/200 [05:14<38:25, 13.33s/it, test_loss=3.63, train_loss=2.93] 14%|█▎        | 27/200 [05:28<38:25, 13.33s/it, test_loss=3.57, train_loss=2.93] 14%|█▍        | 28/200 [05:28<38:29, 13.43s/it, test_loss=3.57, train_loss=2.93] 14%|█▍        | 28/200 [05:41<38:29, 13.43s/it, test_loss=3.55, train_loss=2.94] 14%|█▍        | 29/200 [05:41<38:27, 13.49s/it, test_loss=3.55, train_loss=2.94] 14%|█▍        | 29/200 [05:55<38:27, 13.49s/it, test_loss=3.42, train_loss=2.79] 15%|█▌        | 30/200 [05:55<38:27, 13.57s/it, test_loss=3.42, train_loss=2.79] 15%|█▌        | 30/200 [06:09<38:27, 13.57s/it, test_loss=3.52, train_loss=2.91] 16%|█▌        | 31/200 [06:09<38:19, 13.61s/it, test_loss=3.52, train_loss=2.91] 16%|█▌        | 31/200 [06:23<38:19, 13.61s/it, test_loss=3.37, train_loss=2.8]  16%|█▌        | 32/200 [06:23<38:12, 13.65s/it, test_loss=3.37, train_loss=2.8] 16%|█▌        | 32/200 [06:36<38:12, 13.65s/it, test_loss=3.28, train_loss=2.72] 16%|█▋        | 33/200 [06:36<38:08, 13.70s/it, test_loss=3.28, train_loss=2.72] 16%|█▋        | 33/200 [06:50<38:08, 13.70s/it, test_loss=3.5, train_loss=2.86]  17%|█▋        | 34/200 [06:50<37:55, 13.71s/it, test_loss=3.5, train_loss=2.86] 17%|█▋        | 34/200 [07:04<37:55, 13.71s/it, test_loss=3.29, train_loss=2.7] 18%|█▊        | 35/200 [07:04<37:41, 13.70s/it, test_loss=3.29, train_loss=2.7] 18%|█▊        | 35/200 [07:18<37:41, 13.70s/it, test_loss=3.26, train_loss=2.74] 18%|█▊        | 36/200 [07:18<37:28, 13.71s/it, test_loss=3.26, train_loss=2.74] 18%|█▊        | 36/200 [07:31<37:28, 13.71s/it, test_loss=3.28, train_loss=2.66] 18%|█▊        | 37/200 [07:31<37:12, 13.70s/it, test_loss=3.28, train_loss=2.66] 18%|█▊        | 37/200 [07:45<37:12, 13.70s/it, test_loss=3.2, train_loss=2.67]  19%|█▉        | 38/200 [07:45<37:04, 13.73s/it, test_loss=3.2, train_loss=2.67] 19%|█▉        | 38/200 [07:59<37:04, 13.73s/it, test_loss=3.41, train_loss=2.7] 20%|█▉        | 39/200 [07:59<36:55, 13.76s/it, test_loss=3.41, train_loss=2.7] 20%|█▉        | 39/200 [08:13<36:55, 13.76s/it, test_loss=3.37, train_loss=2.61] 20%|██        | 40/200 [08:13<36:56, 13.85s/it, test_loss=3.37, train_loss=2.61] 20%|██        | 40/200 [08:27<36:56, 13.85s/it, test_loss=3.52, train_loss=2.83] 20%|██        | 41/200 [08:27<36:43, 13.86s/it, test_loss=3.52, train_loss=2.83] 20%|██        | 41/200 [08:41<36:43, 13.86s/it, test_loss=3.39, train_loss=2.75] 21%|██        | 42/200 [08:41<36:23, 13.82s/it, test_loss=3.39, train_loss=2.75] 21%|██        | 42/200 [08:55<36:23, 13.82s/it, test_loss=3.22, train_loss=2.63] 22%|██▏       | 43/200 [08:55<36:28, 13.94s/it, test_loss=3.22, train_loss=2.63] 22%|██▏       | 43/200 [09:09<36:28, 13.94s/it, test_loss=3.32, train_loss=2.72] 22%|██▏       | 44/200 [09:09<36:09, 13.91s/it, test_loss=3.32, train_loss=2.72] 22%|██▏       | 44/200 [09:23<36:09, 13.91s/it, test_loss=3.49, train_loss=2.8]  22%|██▎       | 45/200 [09:23<36:05, 13.97s/it, test_loss=3.49, train_loss=2.8] 22%|██▎       | 45/200 [09:37<36:05, 13.97s/it, test_loss=3.38, train_loss=2.72] 23%|██▎       | 46/200 [09:37<35:51, 13.97s/it, test_loss=3.38, train_loss=2.72] 23%|██▎       | 46/200 [09:51<35:51, 13.97s/it, test_loss=3.2, train_loss=2.61]  24%|██▎       | 47/200 [09:51<35:43, 14.01s/it, test_loss=3.2, train_loss=2.61] 24%|██▎       | 47/200 [10:05<35:43, 14.01s/it, test_loss=3.68, train_loss=2.97] 24%|██▍       | 48/200 [10:05<35:29, 14.01s/it, test_loss=3.68, train_loss=2.97] 24%|██▍       | 48/200 [10:19<35:29, 14.01s/it, test_loss=3.22, train_loss=2.58] 24%|██▍       | 49/200 [10:19<35:11, 13.98s/it, test_loss=3.22, train_loss=2.58] 24%|██▍       | 49/200 [10:33<35:11, 13.98s/it, test_loss=3.66, train_loss=2.94] 25%|██▌       | 50/200 [10:33<34:52, 13.95s/it, test_loss=3.66, train_loss=2.94] 25%|██▌       | 50/200 [10:46<34:52, 13.95s/it, test_loss=3.08, train_loss=2.57] 26%|██▌       | 51/200 [10:46<34:32, 13.91s/it, test_loss=3.08, train_loss=2.57] 26%|██▌       | 51/200 [11:01<34:32, 13.91s/it, test_loss=3.38, train_loss=2.66] 26%|██▌       | 52/200 [11:01<34:28, 13.97s/it, test_loss=3.38, train_loss=2.66] 26%|██▌       | 52/200 [11:15<34:28, 13.97s/it, test_loss=3.24, train_loss=2.53] 26%|██▋       | 53/200 [11:15<34:27, 14.06s/it, test_loss=3.24, train_loss=2.53] 26%|██▋       | 53/200 [11:29<34:27, 14.06s/it, test_loss=3.23, train_loss=2.56] 27%|██▋       | 54/200 [11:29<34:25, 14.15s/it, test_loss=3.23, train_loss=2.56] 27%|██▋       | 54/200 [11:43<34:25, 14.15s/it, test_loss=3.4, train_loss=2.63]  28%|██▊       | 55/200 [11:43<34:03, 14.09s/it, test_loss=3.4, train_loss=2.63] 28%|██▊       | 55/200 [11:57<34:03, 14.09s/it, test_loss=3.07, train_loss=2.58] 28%|██▊       | 56/200 [11:57<33:53, 14.12s/it, test_loss=3.07, train_loss=2.58] 28%|██▊       | 56/200 [12:11<33:53, 14.12s/it, test_loss=3.38, train_loss=2.66] 28%|██▊       | 57/200 [12:11<33:32, 14.07s/it, test_loss=3.38, train_loss=2.66] 28%|██▊       | 57/200 [12:25<33:32, 14.07s/it, test_loss=3.24, train_loss=2.53] 29%|██▉       | 58/200 [12:25<33:20, 14.09s/it, test_loss=3.24, train_loss=2.53] 29%|██▉       | 58/200 [12:39<33:20, 14.09s/it, test_loss=3.25, train_loss=2.54] 30%|██▉       | 59/200 [12:39<32:59, 14.04s/it, test_loss=3.25, train_loss=2.54] 30%|██▉       | 59/200 [12:53<32:59, 14.04s/it, test_loss=3.25, train_loss=2.58] 30%|███       | 60/200 [12:53<32:37, 13.98s/it, test_loss=3.25, train_loss=2.58] 30%|███       | 60/200 [13:07<32:37, 13.98s/it, test_loss=3.21, train_loss=2.6]  30%|███       | 61/200 [13:07<32:26, 14.00s/it, test_loss=3.21, train_loss=2.6] 30%|███       | 61/200 [13:21<32:26, 14.00s/it, test_loss=3.04, train_loss=2.57] 31%|███       | 62/200 [13:21<32:14, 14.02s/it, test_loss=3.04, train_loss=2.57] 31%|███       | 62/200 [13:35<32:14, 14.02s/it, test_loss=3.21, train_loss=2.56] 32%|███▏      | 63/200 [13:35<31:58, 14.00s/it, test_loss=3.21, train_loss=2.56] 32%|███▏      | 63/200 [13:49<31:58, 14.00s/it, test_loss=3.38, train_loss=2.67] 32%|███▏      | 64/200 [13:49<31:38, 13.96s/it, test_loss=3.38, train_loss=2.67] 32%|███▏      | 64/200 [14:03<31:38, 13.96s/it, test_loss=3.86, train_loss=3.15] 32%|███▎      | 65/200 [14:03<31:30, 14.00s/it, test_loss=3.86, train_loss=3.15] 32%|███▎      | 65/200 [14:17<31:30, 14.00s/it, test_loss=3.39, train_loss=2.67] 33%|███▎      | 66/200 [14:17<31:15, 14.00s/it, test_loss=3.39, train_loss=2.67] 33%|███▎      | 66/200 [14:31<31:15, 14.00s/it, test_loss=3.2, train_loss=2.58]  34%|███▎      | 67/200 [14:31<30:59, 13.98s/it, test_loss=3.2, train_loss=2.58] 34%|███▎      | 67/200 [14:45<30:59, 13.98s/it, test_loss=3.19, train_loss=2.55] 34%|███▍      | 68/200 [14:45<30:43, 13.96s/it, test_loss=3.19, train_loss=2.55] 34%|███▍      | 68/200 [14:59<30:43, 13.96s/it, test_loss=3.47, train_loss=2.76] 34%|███▍      | 69/200 [14:59<30:22, 13.91s/it, test_loss=3.47, train_loss=2.76] 34%|███▍      | 69/200 [15:13<30:22, 13.91s/it, test_loss=3.27, train_loss=2.56] 35%|███▌      | 70/200 [15:13<30:18, 13.99s/it, test_loss=3.27, train_loss=2.56] 35%|███▌      | 70/200 [15:27<30:18, 13.99s/it, test_loss=3.17, train_loss=2.55] 36%|███▌      | 71/200 [15:27<30:03, 13.98s/it, test_loss=3.17, train_loss=2.55] 36%|███▌      | 71/200 [15:41<30:03, 13.98s/it, test_loss=3.39, train_loss=2.68] 36%|███▌      | 72/200 [15:41<29:46, 13.96s/it, test_loss=3.39, train_loss=2.68] 36%|███▌      | 72/200 [15:55<29:46, 13.96s/it, test_loss=3.21, train_loss=2.54] 36%|███▋      | 73/200 [15:55<29:32, 13.96s/it, test_loss=3.21, train_loss=2.54] 36%|███▋      | 73/200 [16:09<29:32, 13.96s/it, test_loss=3.39, train_loss=2.69] 37%|███▋      | 74/200 [16:09<29:21, 13.98s/it, test_loss=3.39, train_loss=2.69] 37%|███▋      | 74/200 [16:23<29:21, 13.98s/it, test_loss=3.11, train_loss=2.56] 38%|███▊      | 75/200 [16:23<29:03, 13.95s/it, test_loss=3.11, train_loss=2.56] 38%|███▊      | 75/200 [16:37<29:03, 13.95s/it, test_loss=3.19, train_loss=2.54] 38%|███▊      | 76/200 [16:37<28:51, 13.96s/it, test_loss=3.19, train_loss=2.54] 38%|███▊      | 76/200 [16:51<28:51, 13.96s/it, test_loss=3.2, train_loss=2.55]  38%|███▊      | 77/200 [16:51<28:37, 13.96s/it, test_loss=3.2, train_loss=2.55] 38%|███▊      | 77/200 [17:05<28:37, 13.96s/it, test_loss=3.14, train_loss=2.58] 39%|███▉      | 78/200 [17:05<28:26, 13.99s/it, test_loss=3.14, train_loss=2.58] 39%|███▉      | 78/200 [17:19<28:26, 13.99s/it, test_loss=3.17, train_loss=2.56] 40%|███▉      | 79/200 [17:19<28:03, 13.91s/it, test_loss=3.17, train_loss=2.56] 40%|███▉      | 79/200 [17:32<28:03, 13.91s/it, test_loss=3.35, train_loss=2.6]  40%|████      | 80/200 [17:32<27:44, 13.87s/it, test_loss=3.35, train_loss=2.6] 40%|████      | 80/200 [17:46<27:44, 13.87s/it, test_loss=3.33, train_loss=2.58] 40%|████      | 81/200 [17:46<27:32, 13.89s/it, test_loss=3.33, train_loss=2.58] 40%|████      | 81/200 [18:00<27:32, 13.89s/it, test_loss=3.21, train_loss=2.51] 41%|████      | 82/200 [18:00<27:26, 13.95s/it, test_loss=3.21, train_loss=2.51] 41%|████      | 82/200 [18:14<27:26, 13.95s/it, test_loss=3.18, train_loss=2.52] 42%|████▏     | 83/200 [18:14<27:08, 13.91s/it, test_loss=3.18, train_loss=2.52] 42%|████▏     | 83/200 [18:28<27:08, 13.91s/it, test_loss=3.06, train_loss=2.55] 42%|████▏     | 84/200 [18:28<26:59, 13.96s/it, test_loss=3.06, train_loss=2.55] 42%|████▏     | 84/200 [18:42<26:59, 13.96s/it, test_loss=3.38, train_loss=2.67] 42%|████▎     | 85/200 [18:42<26:42, 13.94s/it, test_loss=3.38, train_loss=2.67] 42%|████▎     | 85/200 [18:56<26:42, 13.94s/it, test_loss=3.18, train_loss=2.55] 43%|████▎     | 86/200 [18:56<26:20, 13.86s/it, test_loss=3.18, train_loss=2.55] 43%|████▎     | 86/200 [19:10<26:20, 13.86s/it, test_loss=3.14, train_loss=2.56] 44%|████▎     | 87/200 [19:10<26:07, 13.87s/it, test_loss=3.14, train_loss=2.56] 44%|████▎     | 87/200 [19:24<26:07, 13.87s/it, test_loss=3.03, train_loss=2.53] 44%|████▍     | 88/200 [19:24<25:57, 13.91s/it, test_loss=3.03, train_loss=2.53] 44%|████▍     | 88/200 [19:38<25:57, 13.91s/it, test_loss=3.17, train_loss=2.55] 44%|████▍     | 89/200 [19:38<25:47, 13.94s/it, test_loss=3.17, train_loss=2.55] 44%|████▍     | 89/200 [19:52<25:47, 13.94s/it, test_loss=3.48, train_loss=2.74] 45%|████▌     | 90/200 [19:52<25:29, 13.90s/it, test_loss=3.48, train_loss=2.74] 45%|████▌     | 90/200 [20:06<25:29, 13.90s/it, test_loss=3.17, train_loss=2.54] 46%|████▌     | 91/200 [20:06<25:18, 13.93s/it, test_loss=3.17, train_loss=2.54] 46%|████▌     | 91/200 [20:19<25:18, 13.93s/it, test_loss=3.18, train_loss=2.52] 46%|████▌     | 92/200 [20:19<24:58, 13.88s/it, test_loss=3.18, train_loss=2.52] 46%|████▌     | 92/200 [20:33<24:58, 13.88s/it, test_loss=3.38, train_loss=2.66] 46%|████▋     | 93/200 [20:33<24:39, 13.83s/it, test_loss=3.38, train_loss=2.66] 46%|████▋     | 93/200 [20:39<23:45, 13.32s/it, test_loss=3.38, train_loss=2.66]
Traceback (most recent call last):
  File "/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py", line 215, in <module>
    model, train_loss_ia, test_loss_ia = train_network(model_ia, X_train, X_test, y_train, y_test,w_train,w_test, train_hp=train_hp)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py", line 170, in train_network
    raise ValueError("Loss is NaN, terminating training")
ValueError: Loss is NaN, terminating training
