xgb_Wd_sftmx_md10_x7x3_5e3_4
{'lr': 0.0001, 'batch_size': 5000, 'BS': 20000, 'N_epochs': 200, 'seed': 0}
>> Training...
  0%|          | 0/200 [00:00<?, ?it/s]/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  theta = torch.tensor(theta_init)
/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(out)
  0%|          | 0/200 [00:13<?, ?it/s, test_loss=21.2, train_loss=17.3]  0%|          | 1/200 [00:13<45:25, 13.70s/it, test_loss=21.2, train_loss=17.3]  0%|          | 1/200 [00:27<45:25, 13.70s/it, test_loss=18, train_loss=14.6]    1%|          | 2/200 [00:27<44:30, 13.49s/it, test_loss=18, train_loss=14.6]  1%|          | 2/200 [00:40<44:30, 13.49s/it, test_loss=15.5, train_loss=12.6]  2%|▏         | 3/200 [00:40<44:09, 13.45s/it, test_loss=15.5, train_loss=12.6]  2%|▏         | 3/200 [00:53<44:09, 13.45s/it, test_loss=11.5, train_loss=9.39]  2%|▏         | 4/200 [00:53<43:58, 13.46s/it, test_loss=11.5, train_loss=9.39]  2%|▏         | 4/200 [01:07<43:58, 13.46s/it, test_loss=5.89, train_loss=4.58]  2%|▎         | 5/200 [01:07<43:20, 13.34s/it, test_loss=5.89, train_loss=4.58]  2%|▎         | 5/200 [01:20<43:20, 13.34s/it, test_loss=3.93, train_loss=3.3]   3%|▎         | 6/200 [01:20<43:08, 13.34s/it, test_loss=3.93, train_loss=3.3]  3%|▎         | 6/200 [01:33<43:08, 13.34s/it, test_loss=4.28, train_loss=3.46]  4%|▎         | 7/200 [01:33<42:53, 13.34s/it, test_loss=4.28, train_loss=3.46]  4%|▎         | 7/200 [01:47<42:53, 13.34s/it, test_loss=3.77, train_loss=2.97]  4%|▍         | 8/200 [01:47<42:40, 13.33s/it, test_loss=3.77, train_loss=2.97]  4%|▍         | 8/200 [02:00<42:40, 13.33s/it, test_loss=3.82, train_loss=3.22]  4%|▍         | 9/200 [02:00<42:18, 13.29s/it, test_loss=3.82, train_loss=3.22]  4%|▍         | 9/200 [02:13<42:18, 13.29s/it, test_loss=3.55, train_loss=2.92]  5%|▌         | 10/200 [02:13<41:52, 13.23s/it, test_loss=3.55, train_loss=2.92]  5%|▌         | 10/200 [02:26<41:52, 13.23s/it, test_loss=3.72, train_loss=3.04]  6%|▌         | 11/200 [02:26<41:26, 13.15s/it, test_loss=3.72, train_loss=3.04]  6%|▌         | 11/200 [02:39<41:26, 13.15s/it, test_loss=3.7, train_loss=3.02]   6%|▌         | 12/200 [02:39<41:05, 13.11s/it, test_loss=3.7, train_loss=3.02]  6%|▌         | 12/200 [02:52<41:05, 13.11s/it, test_loss=3.64, train_loss=3.04]  6%|▋         | 13/200 [02:52<40:56, 13.13s/it, test_loss=3.64, train_loss=3.04]  6%|▋         | 13/200 [03:05<40:56, 13.13s/it, test_loss=3.62, train_loss=3.02]  7%|▋         | 14/200 [03:05<40:54, 13.19s/it, test_loss=3.62, train_loss=3.02]  7%|▋         | 14/200 [03:19<40:54, 13.19s/it, test_loss=3.54, train_loss=2.97]  8%|▊         | 15/200 [03:19<40:43, 13.21s/it, test_loss=3.54, train_loss=2.97]  8%|▊         | 15/200 [03:32<40:43, 13.21s/it, test_loss=3.72, train_loss=3.01]  8%|▊         | 16/200 [03:32<40:24, 13.18s/it, test_loss=3.72, train_loss=3.01]  8%|▊         | 16/200 [03:45<40:24, 13.18s/it, test_loss=3.89, train_loss=3.17]  8%|▊         | 17/200 [03:45<40:03, 13.13s/it, test_loss=3.89, train_loss=3.17]  8%|▊         | 17/200 [03:58<40:03, 13.13s/it, test_loss=3.84, train_loss=3.15]  9%|▉         | 18/200 [03:58<39:49, 13.13s/it, test_loss=3.84, train_loss=3.15]  9%|▉         | 18/200 [04:11<39:49, 13.13s/it, test_loss=3.68, train_loss=3]    10%|▉         | 19/200 [04:11<39:32, 13.11s/it, test_loss=3.68, train_loss=3] 10%|▉         | 19/200 [04:24<39:32, 13.11s/it, test_loss=3.56, train_loss=2.88] 10%|█         | 20/200 [04:24<39:27, 13.15s/it, test_loss=3.56, train_loss=2.88] 10%|█         | 20/200 [04:25<39:45, 13.25s/it, test_loss=3.56, train_loss=2.88]
Traceback (most recent call last):
  File "/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py", line 215, in <module>
    model, train_loss_ia, test_loss_ia = train_network(model_ia, X_train, X_test, y_train, y_test,w_train,w_test, train_hp=train_hp)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py", line 170, in train_network
    raise ValueError("Loss is NaN, terminating training")
ValueError: Loss is NaN, terminating training
