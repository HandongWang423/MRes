xgb_Wd_sftmx_md10_x7x3_9e3
{'lr': 0.001, 'batch_size': 9000, 'N_epochs': 200, 'seed': 0}
>> Training...
  0%|          | 0/200 [00:00<?, ?it/s]/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  theta = torch.tensor(theta_init)
/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py:72: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(out)
  0%|          | 0/200 [00:16<?, ?it/s, test_loss=4.29, train_loss=3.52]  0%|          | 1/200 [00:16<55:04, 16.61s/it, test_loss=4.29, train_loss=3.52]  0%|          | 1/200 [00:33<55:04, 16.61s/it, test_loss=11.4, train_loss=9.32]  1%|          | 2/200 [00:33<54:57, 16.65s/it, test_loss=11.4, train_loss=9.32]  1%|          | 2/200 [00:44<1:13:07, 22.16s/it, test_loss=11.4, train_loss=9.32]
Traceback (most recent call last):
  File "/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py", line 211, in <module>
    model, train_loss_ia, test_loss_ia = train_network(model_ia, X_train, X_test, y_train, y_test,w_train,w_test, train_hp=train_hp)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py", line 166, in train_network
    raise ValueError("Loss is NaN, terminating training")
ValueError: Loss is NaN, terminating training
