{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import mplhep as hep\n",
    "from tqdm import tqdm\n",
    "import  matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.set_option('display.max_columns', 150)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from iminuit import Minuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the parameters and predefined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetcode = 'x30x20'\n",
    "filecode = '_InfAwar_test_40_200'\n",
    "\n",
    "train_hp = {\n",
    "    \"lr\":0.003,\n",
    "    \"batch_size\":20000,\n",
    "    \"N_epochs\":200,\n",
    "    \"seed\":0,\n",
    "    'eplim':40\n",
    "}\n",
    "MInum = 140\n",
    "nodes = [100,100]\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "set_seed(train_hp['seed'])\n",
    "\n",
    "def hess_to_tensor(H):\n",
    "    hess_elements = []\n",
    "    for i in range(len(H)):\n",
    "        for j in range(len(H)):\n",
    "            hess_elements.append(H[i][j].reshape(1))\n",
    "    return torch.cat(hess_elements).reshape(len(H),len(H))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Net\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_features=40, nodes=[100,100], output_nodes=5):\n",
    "        super(Net, self).__init__()\n",
    "        # Build network\n",
    "        n_nodes = [n_features] + nodes + [output_nodes]\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(n_nodes)-1):\n",
    "            self.layers.append(nn.Linear(n_nodes[i], n_nodes[i+1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers[0](x)\n",
    "        for layer in self.layers[1:]:\n",
    "            out = layer(out)\n",
    "        # Apply softmax\n",
    "        return torch.softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Inference Aware Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfAwareLoss(nn.Module):\n",
    "    def __init__(self,weight,i,epoch):\n",
    "        super(InfAwareLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.i = i\n",
    "        self.epoch =epoch\n",
    "    \n",
    "    def forward(self,input,target):\n",
    "        while self.i > self.epoch:\n",
    "            # Input = torch.tensor(input)\n",
    "            # Target = torch.tensor(target,dtype=torch.int8)\n",
    "            label = torch.argmax(target,dim=1)\n",
    "            pred = torch.argmax(input,dim=1)\n",
    "            # plt.hist(pred)\n",
    "            plt.show()\n",
    "            weight = torch.tensor(self.weight.values)\n",
    "            cm = torch.zeros(7,7)\n",
    "            for t, p, w in zip(label.view(-1), pred.view(-1), weight.view(-1)):\n",
    "                cm[p,t] += 1\n",
    "            cm =cm[1:, :]\n",
    "            O = cm.sum(dim=1)\n",
    "            # print(cm)\n",
    "            # print(O)\n",
    "            def NLL(mu):\n",
    "                mu0 =torch.tensor([1.0])\n",
    "                theta = torch.cat((mu0,mu))\n",
    "                return -(O@(torch.log(cm@theta+1e-3))-(cm@theta).sum())\n",
    "            mu = torch.tensor([1.0,1.0,1.0,1.0,1.0,1.0],requires_grad=True)\n",
    "            hess = torch.func.hessian(NLL)(mu)\n",
    "            # print(hess)\n",
    "            # print(torch.det(hess))\n",
    "            I = torch.inverse(hess_to_tensor(hess))\n",
    "            # print(hess)\n",
    "            loss = torch.trace(I)**0.5 \n",
    "            # print(loss)\n",
    "            return loss.clone().detach().requires_grad_(True)\n",
    "        else:\n",
    "            return torch.tensor([0.0],requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Trainnning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the trainning function\n",
    "from NNfunctions import get_batches, get_total_loss\n",
    "def train_network_cross_entropy(model, X_train,X_test,y_train,y_test,weight, train_hp={}):\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=train_hp[\"lr\"])\n",
    "\n",
    "    X_train =X_train.to_numpy()\n",
    "    X_test = X_test.to_numpy()\n",
    "    y_train = y_train.to_numpy()\n",
    "    y_test = y_test.to_numpy()\n",
    "    \n",
    "    \n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    train_loss, test_loss = [], []\n",
    "    i = 0\n",
    "    eplim = train_hp['eplim']\n",
    "    epchs = train_hp['N_epochs']\n",
    "\n",
    "    print(\">> Training...\")\n",
    "    with tqdm(range(train_hp[\"N_epochs\"])) as t:\n",
    "        for i_epoch in t:\n",
    "            model.train()\n",
    "            # print(i)\n",
    "            # \"get_batches\": function defined in statml_tools.py to separate the training data into batches\n",
    "            batch_gen = get_batches([X_train, y_train], batch_size=train_hp['batch_size'],\n",
    "                                    randomise=True, include_remainder=False\n",
    "                                )\n",
    "            ia_loss = InfAwareLoss(weight,i,eplim)\n",
    "            for X_tensor, y_tensor in batch_gen:\n",
    "                optimiser.zero_grad()\n",
    "                output = model(X_tensor)\n",
    "                # print(output)\n",
    "                ia = ia_loss(output, y_tensor)\n",
    "                ce = ce_loss(output, y_tensor)\n",
    "                \n",
    "                loss = 10*i/epchs*ia + (1-i/epchs)*ce\n",
    "                # while i>eplim:\n",
    "                #     print(ia.detach().numpy(),ce.detach().numpy())\n",
    "                loss.backward()\n",
    "                # print(loss)\n",
    "                optimiser.step()\n",
    "\n",
    "            model.eval()\n",
    "            # if i > eplim:\n",
    "            #     # print('Yes')\n",
    "            #     Loss = ia_loss\n",
    "            # else:\n",
    "            #     Loss = ce_loss\n",
    "            Loss = ce_loss\n",
    "            # \"get_total_loss\": function defined in statml_tools.py to evaluate the network in batches (useful for large datasets)\n",
    "            train_loss.append(get_total_loss(model, Loss, X_train, y_train))\n",
    "            test_loss.append(get_total_loss(model, Loss, X_test, y_test))\n",
    "            t.set_postfix(train_loss=train_loss[-1], test_loss=test_loss[-1])\n",
    "            i+=1\n",
    "\n",
    "    print(\">> Training finished\")\n",
    "    model.eval()\n",
    "\n",
    "    return model, train_loss, test_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The original trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('/vols/cms/hw423/Data/Week14/df_InfA_RD_DPrmvd_Bld.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_series = pd.read_csv('/vols/cms/hw423/Week6/MI_balanced.csv')\n",
    "MIcol = mi_series.head(MInum)['Features']\n",
    "model_ia = Net(n_features=MInum, nodes=nodes, output_nodes=7)\n",
    "dfx = df[MIcol]\n",
    "dfy =  df.iloc[:,-7:]\n",
    "dfw = df['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dfx, dfy.loc[dfx.index], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [43:46<00:00, 13.13s/it, test_loss=1.59, train_loss=1.58]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Training finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_bce, train_loss_bce, test_loss_bce = train_network_cross_entropy(model_ia, X_train, X_test, y_train, y_test, train_hp=train_hp,weight=dfw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "del df\n",
    "data = pd.read_parquet('/vols/cms/hw423/Data/Week14/df_InfA_RD_DPrmvd.parquet')\n",
    "data = data[MIcol]\n",
    "col = ['$\\gamma\\gamma$','ggH','qqH','WH','ZH','ttH','tH']\n",
    "oc= model_bce(torch.tensor(data.to_numpy(),dtype=torch.float32))\n",
    "octest =pd.DataFrame(oc.detach(), columns = col, index = data.index)\n",
    "np.save(f'/vols/cms/hw423/Data/Week14/octest_{filecode}_1.npy', np.array(octest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ia = Net(n_features=7, nodes=nodes, output_nodes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['$\\gamma\\gamma$','ggH','qqH','WH','ZH','ttH','tH']\n",
    "oc = np.load('/vols/cms/hw423/Data/Week12/octest_x30x20_2.npy')\n",
    "y = pd.read_pickle(f'/vols/cms/hw423/Data/Week12/Label.pkl').astype(int)\n",
    "dfx = pd.DataFrame(oc,columns = col)\n",
    "dfy = pd.get_dummies(y)\n",
    "dfw = pd.DataFrame(np.load('/vols/cms/hw423/Data/Week7/dfw.npy'),columns=['weight'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = dfx.sample(n=100000)\n",
    "dfy=dfy.iloc[dfx.index,:]\n",
    "dfw = dfw.iloc[dfx.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dfx,dfy, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 21/60 [00:53<01:37,  2.50s/it, test_loss=1.42, train_loss=1.42]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1500e+02, 1.1530e+03, 5.3400e+02, 1.4000e+01, 1.6000e+01, 3.0000e+00,\n",
      "         8.4000e+01],\n",
      "        [7.8000e+01, 1.7200e+02, 8.7100e+02, 3.0000e+00, 2.0000e+00, 2.0000e+00,\n",
      "         9.7000e+01],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.3700e+02, 9.9000e+01, 2.3000e+02, 5.6000e+01, 9.0000e+00, 3.3900e+02,\n",
      "         2.2250e+03]])\n",
      "tensor(nan, grad_fn=<PowBackward0>)\n",
      "tensor([[3.1200e+02, 1.2100e+03, 5.5300e+02, 2.0000e+01, 1.8000e+01, 4.0000e+00,\n",
      "         7.5000e+01],\n",
      "        [9.4000e+01, 1.4600e+02, 8.5100e+02, 1.0000e+00, 2.0000e+00, 2.0000e+00,\n",
      "         1.0500e+02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.4400e+02, 7.5000e+01, 1.9300e+02, 4.1000e+01, 9.0000e+00, 3.2200e+02,\n",
      "         2.2630e+03]])\n",
      "tensor(nan, grad_fn=<PowBackward0>)\n",
      "tensor([[2.9300e+02, 1.1730e+03, 5.6700e+02, 1.5000e+01, 1.6000e+01, 3.0000e+00,\n",
      "         8.1000e+01],\n",
      "        [9.4000e+01, 1.6800e+02, 8.6800e+02, 4.0000e+00, 2.0000e+00, 0.0000e+00,\n",
      "         1.0900e+02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.2700e+02, 7.5000e+01, 1.9900e+02, 5.2000e+01, 7.0000e+00, 3.6400e+02,\n",
      "         2.1880e+03]])\n",
      "tensor(nan, grad_fn=<PowBackward0>)\n",
      "tensor([[2.8200e+02, 1.1450e+03, 6.0000e+02, 1.4000e+01, 2.5000e+01, 2.0000e+00,\n",
      "         9.1000e+01],\n",
      "        [6.3000e+01, 1.9000e+02, 8.7100e+02, 5.0000e+00, 4.0000e+00, 4.0000e+00,\n",
      "         1.0600e+02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.3900e+02, 9.3000e+01, 2.0900e+02, 4.4000e+01, 9.0000e+00, 3.5500e+02,\n",
      "         2.2590e+03]])\n",
      "tensor(nan, grad_fn=<PowBackward0>)\n",
      "tensor([[2.8500e+02, 1.1870e+03, 5.5700e+02, 1.2000e+01, 2.6000e+01, 3.0000e+00,\n",
      "         9.3000e+01],\n",
      "        [1.0300e+02, 1.7500e+02, 8.6000e+02, 2.0000e+00, 0.0000e+00, 2.0000e+00,\n",
      "         1.0300e+02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.2300e+02, 8.7000e+01, 1.7700e+02, 3.5000e+01, 3.0000e+00, 3.2800e+02,\n",
      "         2.3360e+03]])\n",
      "tensor(nan, grad_fn=<PowBackward0>)\n",
      "tensor([[3.1000e+02, 1.2040e+03, 5.5800e+02, 1.2000e+01, 1.9000e+01, 3.0000e+00,\n",
      "         8.8000e+01],\n",
      "        [8.9000e+01, 1.7700e+02, 8.5800e+02, 4.0000e+00, 2.0000e+00, 0.0000e+00,\n",
      "         1.0200e+02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.4500e+02, 8.1000e+01, 2.0700e+02, 4.8000e+01, 1.1000e+01, 3.3800e+02,\n",
      "         2.1810e+03]])\n",
      "tensor(nan, grad_fn=<PowBackward0>)\n",
      "tensor([[3.1200e+02, 1.1940e+03, 5.5800e+02, 1.3000e+01, 2.7000e+01, 3.0000e+00,\n",
      "         8.2000e+01],\n",
      "        [9.5000e+01, 1.6900e+02, 8.1200e+02, 4.0000e+00, 2.0000e+00, 1.0000e+00,\n",
      "         1.1600e+02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.4200e+02, 9.6000e+01, 2.0100e+02, 5.7000e+01, 5.0000e+00, 3.4700e+02,\n",
      "         2.2930e+03]])\n",
      "tensor(nan, grad_fn=<PowBackward0>)\n",
      "tensor([[2.7600e+02, 1.1850e+03, 5.4800e+02, 1.0000e+01, 2.3000e+01, 2.0000e+00,\n",
      "         7.8000e+01],\n",
      "        [8.4000e+01, 1.7400e+02, 8.9700e+02, 9.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "         1.2200e+02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.0900e+02, 9.3000e+01, 1.9300e+02, 5.6000e+01, 1.2000e+01, 3.4900e+02,\n",
      "         2.2510e+03]])\n",
      "tensor(nan, grad_fn=<PowBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 22/60 [00:59<02:13,  3.52s/it, test_loss=1.42, train_loss=1.42]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.9200e+02, 1.1250e+03, 5.7800e+02, 1.6000e+01, 3.3000e+01, 3.0000e+00,\n",
      "         8.2000e+01],\n",
      "        [9.2000e+01, 1.7400e+02, 8.5500e+02, 3.0000e+00, 2.0000e+00, 1.0000e+00,\n",
      "         1.0600e+02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.2200e+02, 7.8000e+01, 2.0900e+02, 4.5000e+01, 6.0000e+00, 3.5900e+02,\n",
      "         2.2560e+03]])\n",
      "tensor(nan, grad_fn=<PowBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 22/60 [01:00<01:44,  2.76s/it, test_loss=1.42, train_loss=1.42]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.7800e+02, 1.1360e+03, 5.6100e+02, 1.5000e+01, 1.9000e+01, 1.0000e+00,\n",
      "         7.8000e+01],\n",
      "        [9.2000e+01, 1.7900e+02, 9.0800e+02, 4.0000e+00, 3.0000e+00, 2.0000e+00,\n",
      "         1.0700e+02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.3800e+02, 8.1000e+01, 2.0100e+02, 5.1000e+01, 9.0000e+00, 3.0200e+02,\n",
      "         2.2280e+03]])\n",
      "tensor(nan, grad_fn=<PowBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_bce, train_loss_bce, test_loss_bce \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_network_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_ia\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_hp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_hp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdfw\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 35\u001b[0m, in \u001b[0;36mtrain_network_cross_entropy\u001b[0;34m(model, X_train, X_test, y_train, y_test, weight, train_hp)\u001b[0m\n\u001b[1;32m     33\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# print(loss)\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     \u001b[43moptimiser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# if i > eplim:\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#     # print('Yes')\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#     Loss = ia_loss\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#     Loss = ce_loss\u001b[39;00m\n",
      "File \u001b[0;32m/vols/cms/hw423/env/envs/env/lib/python3.11/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/vols/cms/hw423/env/envs/env/lib/python3.11/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/vols/cms/hw423/env/envs/env/lib/python3.11/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/vols/cms/hw423/env/envs/env/lib/python3.11/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vols/cms/hw423/env/envs/env/lib/python3.11/site-packages/torch/optim/adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    389\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    393\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_bce, train_loss_bce, test_loss_bce = train_network_cross_entropy(model_ia, X_train, X_test, y_train, y_test, train_hp=train_hp,weight=dfw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfAwareLoss(nn.Module):\n",
    "    def __init__(self,weight):\n",
    "        super(InfAwareLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "    def forward(self,input,target):\n",
    "        # Input = torch.tensor(input)\n",
    "        # Target = torch.tensor(target,dtype=torch.int8)\n",
    "        label = torch.argmax(target,dim=1)\n",
    "        pred = torch.argmax(input,dim=1)\n",
    "        # plt.hist(pred)\n",
    "        plt.show()\n",
    "        weight = torch.tensor(dfw.values)\n",
    "        cm = torch.zeros(7,7)\n",
    "        for t, p, w in zip(label.view(-1), pred.view(-1), weight.view(-1)):\n",
    "            cm[p,t] += 1\n",
    "        cm =cm[1:, :]\n",
    "        O = cm.sum(dim=1)\n",
    "        print(cm)\n",
    "        # print(O)\n",
    "        def NLL(mu1,mu2,mu3,mu4,mu5,mu6):\n",
    "            theta = torch.tensor([1.0,mu1,mu2,mu3,mu4,mu5,mu6],dtype=torch.float32)\n",
    "            return -(O@(torch.log(cm@theta+1e-3))-(cm@theta).sum())\n",
    "        m = Minuit(NLL, mu1=1, mu2=1, mu3=1, mu4=1, mu5=1, mu6=1)\n",
    "        m.limits = [(0.0, None)] * 6\n",
    "        m.strategy = 2\n",
    "        m.migrad()\n",
    "        # print(m.values)\n",
    "        loss = torch.trace(torch.tensor(m.covariance,dtype=torch.float32))\n",
    "        # print(cm)\n",
    "        return loss.clone().detach().requires_grad_(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m cm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m7\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t, p, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(label\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), pred\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), weight\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m----> 9\u001b[0m     cm[p,t] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m w\n\u001b[1;32m     10\u001b[0m cm \u001b[38;5;241m=\u001b[39mcm[\u001b[38;5;241m1\u001b[39m:, :]\n\u001b[1;32m     11\u001b[0m O \u001b[38;5;241m=\u001b[39m cm\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input = torch.tensor(dfx.values)\n",
    "target = torch.tensor(dfy.values,dtype=torch.int8)\n",
    "label = torch.argmax(target,dim=1)\n",
    "pred = torch.argmax(input,dim=1)\n",
    "weight = torch.tensor(dfw.values)\n",
    "cm = torch.zeros(7,7)\n",
    "for t, p, w in zip(label.view(-1), pred.view(-1), weight.view(-1)):\n",
    "    cm[p,t] += w\n",
    "cm =cm[1:, :]\n",
    "O = cm.sum(dim=1)\n",
    "def NLL(mu1,mu2,mu3,mu4,mu5,mu6):\n",
    "    theta = torch.tensor([1.0,mu1,mu2,mu3,mu4,mu5,mu6],dtype=torch.float32)\n",
    "    return -(O@(torch.log(cm.T@theta))-(cm.T@theta).sum())\n",
    "m = Minuit(NLL, mu1=1, mu2=1, mu3=1, mu4=1, mu5=1, mu6=1)\n",
    "m.limits = [(0.0, None)] * 6\n",
    "m.strategy = 2\n",
    "m.migrad()\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.7366, dtype=torch.float64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.trace(torch.tensor(m.covariance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(581.0189)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.trace(torch.tensor(m.covariance,dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = torch.zeros(6,7)\n",
    "cm[3,:] = torch.ones(7)\n",
    "O = torch.rand(6)\n",
    "t = torch.tensor([1,0,0,0,0,0,0],dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-inf, -inf, -inf, 0., -inf, -inf])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(cm@t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
