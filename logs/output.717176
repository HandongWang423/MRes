xgb_Wd_sftmx_md10_x7x3_5e3_4
{'lr': 0.0001, 'batch_size': 10000, 'BS': 20000, 'N_epochs': 200, 'seed': 0}
>> Training...
  0%|          | 0/200 [00:00<?, ?it/s]/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  theta = torch.tensor(theta_init)
/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(out)
  0%|          | 0/200 [00:13<?, ?it/s, test_loss=21.2, train_loss=17.3]  0%|          | 1/200 [00:13<44:06, 13.30s/it, test_loss=21.2, train_loss=17.3]  0%|          | 1/200 [00:26<44:06, 13.30s/it, test_loss=18, train_loss=14.6]    1%|          | 2/200 [00:26<43:30, 13.18s/it, test_loss=18, train_loss=14.6]  1%|          | 2/200 [00:39<43:30, 13.18s/it, test_loss=15.5, train_loss=12.6]  2%|▏         | 3/200 [00:39<43:51, 13.36s/it, test_loss=15.5, train_loss=12.6]  2%|▏         | 3/200 [00:53<43:51, 13.36s/it, test_loss=11.5, train_loss=9.39]  2%|▏         | 4/200 [00:53<44:02, 13.48s/it, test_loss=11.5, train_loss=9.39]  2%|▏         | 4/200 [01:07<44:02, 13.48s/it, test_loss=5.89, train_loss=4.58]  2%|▎         | 5/200 [01:07<43:44, 13.46s/it, test_loss=5.89, train_loss=4.58]  2%|▎         | 5/200 [01:20<43:44, 13.46s/it, test_loss=3.93, train_loss=3.3]   3%|▎         | 6/200 [01:20<43:25, 13.43s/it, test_loss=3.93, train_loss=3.3]  3%|▎         | 6/200 [01:33<43:25, 13.43s/it, test_loss=4.28, train_loss=3.46]  4%|▎         | 7/200 [01:33<43:08, 13.41s/it, test_loss=4.28, train_loss=3.46]  4%|▎         | 7/200 [01:47<43:08, 13.41s/it, test_loss=3.77, train_loss=2.97]  4%|▍         | 8/200 [01:47<42:53, 13.40s/it, test_loss=3.77, train_loss=2.97]  4%|▍         | 8/200 [02:00<42:53, 13.40s/it, test_loss=3.82, train_loss=3.22]  4%|▍         | 9/200 [02:00<42:34, 13.38s/it, test_loss=3.82, train_loss=3.22]  4%|▍         | 9/200 [02:13<42:34, 13.38s/it, test_loss=3.55, train_loss=2.92]  5%|▌         | 10/200 [02:13<42:21, 13.38s/it, test_loss=3.55, train_loss=2.92]  5%|▌         | 10/200 [02:27<42:21, 13.38s/it, test_loss=3.72, train_loss=3.04]  6%|▌         | 11/200 [02:27<42:10, 13.39s/it, test_loss=3.72, train_loss=3.04]  6%|▌         | 11/200 [02:40<42:10, 13.39s/it, test_loss=3.7, train_loss=3.02]   6%|▌         | 12/200 [02:40<42:04, 13.43s/it, test_loss=3.7, train_loss=3.02]  6%|▌         | 12/200 [02:54<42:04, 13.43s/it, test_loss=3.64, train_loss=3.04]  6%|▋         | 13/200 [02:54<41:46, 13.40s/it, test_loss=3.64, train_loss=3.04]  6%|▋         | 13/200 [03:07<41:46, 13.40s/it, test_loss=3.62, train_loss=3.02]  7%|▋         | 14/200 [03:07<41:24, 13.36s/it, test_loss=3.62, train_loss=3.02]  7%|▋         | 14/200 [03:21<41:24, 13.36s/it, test_loss=3.54, train_loss=2.97]  8%|▊         | 15/200 [03:21<42:01, 13.63s/it, test_loss=3.54, train_loss=2.97]  8%|▊         | 15/200 [03:35<42:01, 13.63s/it, test_loss=3.72, train_loss=3.01]  8%|▊         | 16/200 [03:35<42:20, 13.81s/it, test_loss=3.72, train_loss=3.01]  8%|▊         | 16/200 [03:49<42:20, 13.81s/it, test_loss=3.89, train_loss=3.17]  8%|▊         | 17/200 [03:49<41:50, 13.72s/it, test_loss=3.89, train_loss=3.17]  8%|▊         | 17/200 [04:02<41:50, 13.72s/it, test_loss=3.84, train_loss=3.15]  9%|▉         | 18/200 [04:02<41:13, 13.59s/it, test_loss=3.84, train_loss=3.15]  9%|▉         | 18/200 [04:16<41:13, 13.59s/it, test_loss=3.68, train_loss=3]    10%|▉         | 19/200 [04:16<40:57, 13.58s/it, test_loss=3.68, train_loss=3] 10%|▉         | 19/200 [04:29<40:57, 13.58s/it, test_loss=3.56, train_loss=2.88] 10%|█         | 20/200 [04:29<40:42, 13.57s/it, test_loss=3.56, train_loss=2.88] 10%|█         | 20/200 [04:38<41:45, 13.92s/it, test_loss=3.56, train_loss=2.88]
Traceback (most recent call last):
  File "/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py", line 215, in <module>
    model, train_loss_ia, test_loss_ia = train_network(model_ia, X_train, X_test, y_train, y_test,w_train,w_test, train_hp=train_hp)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py", line 170, in train_network
    raise ValueError("Loss is NaN, terminating training")
ValueError: Loss is NaN, terminating training
