xgb_Wd_sftmx_md10_x7x3_9e3
{'lr': 0.001, 'batch_size': 5000, 'BS': 50000, 'N_epochs': 200, 'seed': 0}
>> Training...
  0%|          | 0/200 [00:00<?, ?it/s]/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  theta = torch.tensor(theta_init)
/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(out)
  0%|          | 0/200 [00:11<?, ?it/s, test_loss=3.7, train_loss=3.23]  0%|          | 1/200 [00:11<38:45, 11.68s/it, test_loss=3.7, train_loss=3.23]  0%|          | 1/200 [00:23<38:45, 11.68s/it, test_loss=6.42, train_loss=5.09]  1%|          | 2/200 [00:23<38:18, 11.61s/it, test_loss=6.42, train_loss=5.09]  1%|          | 2/200 [00:34<38:18, 11.61s/it, test_loss=4.15, train_loss=3.38]  2%|▏         | 3/200 [00:34<37:52, 11.54s/it, test_loss=4.15, train_loss=3.38]  2%|▏         | 3/200 [00:45<37:52, 11.54s/it, test_loss=3.4, train_loss=2.85]   2%|▏         | 4/200 [00:45<37:21, 11.44s/it, test_loss=3.4, train_loss=2.85]  2%|▏         | 4/200 [00:57<37:21, 11.44s/it, test_loss=5.27, train_loss=3.97]  2%|▎         | 5/200 [00:57<37:13, 11.45s/it, test_loss=5.27, train_loss=3.97]  2%|▎         | 5/200 [01:08<37:13, 11.45s/it, test_loss=3.48, train_loss=2.93]  3%|▎         | 6/200 [01:08<36:54, 11.41s/it, test_loss=3.48, train_loss=2.93]  3%|▎         | 6/200 [01:20<36:54, 11.41s/it, test_loss=3.31, train_loss=2.76]  4%|▎         | 7/200 [01:20<36:37, 11.39s/it, test_loss=3.31, train_loss=2.76]  4%|▎         | 7/200 [01:31<36:37, 11.39s/it, test_loss=3.25, train_loss=2.63]  4%|▍         | 8/200 [01:31<36:18, 11.34s/it, test_loss=3.25, train_loss=2.63]  4%|▍         | 8/200 [01:42<36:18, 11.34s/it, test_loss=3.38, train_loss=2.85]  4%|▍         | 9/200 [01:42<36:06, 11.34s/it, test_loss=3.38, train_loss=2.85]  4%|▍         | 9/200 [01:53<36:06, 11.34s/it, test_loss=3.37, train_loss=2.85]  5%|▌         | 10/200 [01:53<35:47, 11.30s/it, test_loss=3.37, train_loss=2.85]  5%|▌         | 10/200 [02:05<35:47, 11.30s/it, test_loss=4.31, train_loss=3.42]  6%|▌         | 11/200 [02:05<35:40, 11.32s/it, test_loss=4.31, train_loss=3.42]  6%|▌         | 11/200 [02:16<35:40, 11.32s/it, test_loss=12.8, train_loss=10.4]  6%|▌         | 12/200 [02:16<35:36, 11.36s/it, test_loss=12.8, train_loss=10.4]  6%|▌         | 12/200 [02:28<35:36, 11.36s/it, test_loss=12.8, train_loss=10.4]  6%|▋         | 13/200 [02:28<35:41, 11.45s/it, test_loss=12.8, train_loss=10.4]  6%|▋         | 13/200 [02:39<35:41, 11.45s/it, test_loss=12.8, train_loss=10.4]  7%|▋         | 14/200 [02:39<35:29, 11.45s/it, test_loss=12.8, train_loss=10.4]  7%|▋         | 14/200 [02:51<35:29, 11.45s/it, test_loss=12.8, train_loss=10.4]  8%|▊         | 15/200 [02:51<35:10, 11.41s/it, test_loss=12.8, train_loss=10.4]  8%|▊         | 15/200 [03:02<35:10, 11.41s/it, test_loss=12.8, train_loss=10.4]  8%|▊         | 16/200 [03:02<35:19, 11.52s/it, test_loss=12.8, train_loss=10.4]  8%|▊         | 16/200 [03:14<35:19, 11.52s/it, test_loss=12.8, train_loss=10.4]  8%|▊         | 17/200 [03:14<35:10, 11.53s/it, test_loss=12.8, train_loss=10.4]  8%|▊         | 17/200 [03:26<35:10, 11.53s/it, test_loss=12.8, train_loss=10.4]  9%|▉         | 18/200 [03:26<35:11, 11.60s/it, test_loss=12.8, train_loss=10.4]  9%|▉         | 18/200 [03:37<35:11, 11.60s/it, test_loss=12.8, train_loss=10.4] 10%|▉         | 19/200 [03:37<35:06, 11.64s/it, test_loss=12.8, train_loss=10.4] 10%|▉         | 19/200 [03:49<35:06, 11.64s/it, test_loss=12.8, train_loss=10.4] 10%|█         | 20/200 [03:49<34:59, 11.66s/it, test_loss=12.8, train_loss=10.4] 10%|█         | 20/200 [03:50<34:33, 11.52s/it, test_loss=12.8, train_loss=10.4]
Traceback (most recent call last):
  File "/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py", line 215, in <module>
    model, train_loss_ia, test_loss_ia = train_network(model_ia, X_train, X_test, y_train, y_test,w_train,w_test, train_hp=train_hp)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py", line 170, in train_network
    raise ValueError("Loss is NaN, terminating training")
ValueError: Loss is NaN, terminating training
