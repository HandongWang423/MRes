xgb_Wd_sftmx_md10_x7x3_5e3_4
{'lr': 0.0001, 'batch_size': 5000, 'BS': 20000, 'N_epochs': 200, 'seed': 0}
>> Training...
  0%|          | 0/200 [00:00<?, ?it/s]/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  theta = torch.tensor(theta_init)
/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(out)
  0%|          | 0/200 [00:13<?, ?it/s, test_loss=21.2, train_loss=17.3]  0%|          | 1/200 [00:13<44:43, 13.48s/it, test_loss=21.2, train_loss=17.3]  0%|          | 1/200 [00:26<44:43, 13.48s/it, test_loss=18, train_loss=14.6]    1%|          | 2/200 [00:26<43:53, 13.30s/it, test_loss=18, train_loss=14.6]  1%|          | 2/200 [00:39<43:53, 13.30s/it, test_loss=15.5, train_loss=12.6]  2%|▏         | 3/200 [00:39<42:17, 12.88s/it, test_loss=15.5, train_loss=12.6]  2%|▏         | 3/200 [00:52<42:17, 12.88s/it, test_loss=11.5, train_loss=9.39]  2%|▏         | 4/200 [00:52<43:03, 13.18s/it, test_loss=11.5, train_loss=9.39]  2%|▏         | 4/200 [01:06<43:03, 13.18s/it, test_loss=5.89, train_loss=4.58]  2%|▎         | 5/200 [01:06<43:06, 13.27s/it, test_loss=5.89, train_loss=4.58]  2%|▎         | 5/200 [01:19<43:06, 13.27s/it, test_loss=3.93, train_loss=3.3]   3%|▎         | 6/200 [01:19<42:52, 13.26s/it, test_loss=3.93, train_loss=3.3]  3%|▎         | 6/200 [01:34<42:52, 13.26s/it, test_loss=4.28, train_loss=3.46]  4%|▎         | 7/200 [01:34<44:16, 13.77s/it, test_loss=4.28, train_loss=3.46]  4%|▎         | 7/200 [01:47<44:16, 13.77s/it, test_loss=3.77, train_loss=2.97]  4%|▍         | 8/200 [01:47<43:30, 13.60s/it, test_loss=3.77, train_loss=2.97]  4%|▍         | 8/200 [02:00<43:30, 13.60s/it, test_loss=3.82, train_loss=3.22]  4%|▍         | 9/200 [02:00<42:54, 13.48s/it, test_loss=3.82, train_loss=3.22]  4%|▍         | 9/200 [02:13<42:54, 13.48s/it, test_loss=3.55, train_loss=2.92]  5%|▌         | 10/200 [02:13<42:31, 13.43s/it, test_loss=3.55, train_loss=2.92]  5%|▌         | 10/200 [02:26<42:31, 13.43s/it, test_loss=3.72, train_loss=3.04]  6%|▌         | 11/200 [02:26<41:55, 13.31s/it, test_loss=3.72, train_loss=3.04]  6%|▌         | 11/200 [02:40<41:55, 13.31s/it, test_loss=3.7, train_loss=3.02]   6%|▌         | 12/200 [02:40<41:39, 13.29s/it, test_loss=3.7, train_loss=3.02]  6%|▌         | 12/200 [02:53<41:39, 13.29s/it, test_loss=3.64, train_loss=3.04]  6%|▋         | 13/200 [02:53<41:19, 13.26s/it, test_loss=3.64, train_loss=3.04]  6%|▋         | 13/200 [03:06<41:19, 13.26s/it, test_loss=3.62, train_loss=3.02]  7%|▋         | 14/200 [03:06<41:03, 13.24s/it, test_loss=3.62, train_loss=3.02]  7%|▋         | 14/200 [03:19<41:03, 13.24s/it, test_loss=3.54, train_loss=2.97]  8%|▊         | 15/200 [03:19<40:48, 13.24s/it, test_loss=3.54, train_loss=2.97]  8%|▊         | 15/200 [03:32<40:48, 13.24s/it, test_loss=3.72, train_loss=3.01]  8%|▊         | 16/200 [03:32<40:25, 13.18s/it, test_loss=3.72, train_loss=3.01]  8%|▊         | 16/200 [03:46<40:25, 13.18s/it, test_loss=3.89, train_loss=3.17]  8%|▊         | 17/200 [03:46<40:26, 13.26s/it, test_loss=3.89, train_loss=3.17]  8%|▊         | 17/200 [03:59<40:26, 13.26s/it, test_loss=3.84, train_loss=3.15]  9%|▉         | 18/200 [03:59<40:21, 13.31s/it, test_loss=3.84, train_loss=3.15]  9%|▉         | 18/200 [04:13<40:21, 13.31s/it, test_loss=3.68, train_loss=3]    10%|▉         | 19/200 [04:13<40:23, 13.39s/it, test_loss=3.68, train_loss=3] 10%|▉         | 19/200 [04:26<40:23, 13.39s/it, test_loss=3.56, train_loss=2.88] 10%|█         | 20/200 [04:26<40:16, 13.42s/it, test_loss=3.56, train_loss=2.88] 10%|█         | 20/200 [04:27<40:04, 13.36s/it, test_loss=3.56, train_loss=2.88]
Traceback (most recent call last):
  File "/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py", line 215, in <module>
    model, train_loss_ia, test_loss_ia = train_network(model_ia, X_train, X_test, y_train, y_test,w_train,w_test, train_hp=train_hp)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py", line 170, in train_network
    raise ValueError("Loss is NaN, terminating training")
ValueError: Loss is NaN, terminating training
