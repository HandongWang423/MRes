{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import mplhep as hep\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.set_option('display.max_columns', 150)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "# pd.set_option('display.max_columns', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "filecode = 'InfA_RD_DPrmvd'\n",
    "# filecode = parquetcode\n",
    "\n",
    "\n",
    "train_hp = {\n",
    "    \"lr\":0.0002,\n",
    "    \"batch_size\":500000,\n",
    "    \"N_epochs\":100,\n",
    "    \"seed\":0,\n",
    "    'eplim':-1\n",
    "}\n",
    "nodes = [20,20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "set_seed(train_hp['seed'])\n",
    "\n",
    "def hess_to_tensor(H):\n",
    "    hess_elements = []\n",
    "    for i in range(len(H)):\n",
    "        for j in range(len(H)):\n",
    "            hess_elements.append(H[i][j].reshape(1))\n",
    "    return torch.cat(hess_elements).reshape(len(H),len(H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Net\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_features=40, nodes=[100,100], output_nodes=5):\n",
    "        super(Net, self).__init__()\n",
    "        # Build network\n",
    "        n_nodes = [n_features] + nodes + [output_nodes]\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(n_nodes)-1):\n",
    "            self.layers.append(nn.Linear(n_nodes[i], n_nodes[i+1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers[0](x)\n",
    "        for layer in self.layers[1:]:\n",
    "            out = layer(out)\n",
    "        # Apply softmax\n",
    "        return torch.softmax(out, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "class InfAwareLoss(nn.Module):\n",
    "    def __init__(self,i,epoch):\n",
    "        super(InfAwareLoss, self).__init__()\n",
    "        self.i = i\n",
    "        self.epoch =epoch\n",
    "    \n",
    "    def forward(self,input,target,weight):\n",
    "        while self.i > self.epoch:\n",
    "            # Input = torch.tensor(input)\n",
    "            # Target = torch.tensor(target,dtype=torch.int8)\n",
    "            \n",
    "            label = torch.argmax(target,dim=1)\n",
    "            pred = torch.argmax(input,dim=1)\n",
    "            cm = torch.zeros(7,7)\n",
    "            cm_clone = cm.clone()\n",
    "            for t, p, w in zip(label.view(-1), pred.view(-1), weight.view(-1)):\n",
    "                cm_clone[p,t] += w\n",
    "            cm = cm_clone\n",
    "            cm =cm[1:, :]\n",
    "            O = cm.sum(dim=1)\n",
    "            def NLL(mu):\n",
    "                mu0 =torch.tensor([1.0])\n",
    "                theta = torch.cat((mu0,mu))\n",
    "                return -(O@(torch.log(cm@theta))-(cm@theta).sum())\n",
    "            mu = torch.tensor([1.0,1.0,1.0,1.0,1.0,1.0])\n",
    "            hess = torch.func.hessian(NLL)(mu)\n",
    "            I = torch.inverse(hess_to_tensor(hess))\n",
    "            loss = torch.trace(I)**0.5 \n",
    "            return loss.clone().detach().requires_grad_(True)\n",
    "        else:\n",
    "            return torch.tensor([0.0],requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the trainning function\n",
    "from NNfunctions import get_batches, get_total_loss\n",
    "def train_network_cross_entropy(model, X_train,X_test,y_train,y_test,w_train,w_test, train_hp={}):\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=train_hp[\"lr\"])\n",
    "    X_train =X_train.to_numpy()\n",
    "    X_test = X_test.to_numpy()\n",
    "    y_train = y_train.to_numpy()\n",
    "    y_test = y_test.to_numpy()\n",
    "    w_train = w_train.to_numpy()\n",
    "    w_test = w_test.to_numpy()\n",
    "    \n",
    "    \n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    train_loss, test_loss = [], []\n",
    "    i = 0\n",
    "    eplim = train_hp['eplim']\n",
    "    epchs = train_hp['N_epochs']\n",
    "\n",
    "    print(\">> Training...\")\n",
    "    with tqdm(range(train_hp[\"N_epochs\"])) as t:\n",
    "        for i_epoch in t:\n",
    "            model.train()\n",
    "            # print(i)\n",
    "            # \"get_batches\": function defined in statml_tools.py to separate the training data into batches\n",
    "            batch_gen = get_batches([X_train, y_train, w_train], batch_size=train_hp['batch_size'],\n",
    "                                    randomise=True, include_remainder=False\n",
    "                                )\n",
    "            ia_loss = InfAwareLoss(i,eplim)\n",
    "            for X_tensor, y_tensor, w_tensor in batch_gen:\n",
    "                optimiser.zero_grad()\n",
    "                output = model(X_tensor)\n",
    "                # print(output)\n",
    "                ia = ia_loss(output, y_tensor, w_tensor)\n",
    "                ce = ce_loss(output, y_tensor)\n",
    "                \n",
    "                if i <= eplim:\n",
    "                    loss = ce\n",
    "                else:\n",
    "                    loss = ia\n",
    "                loss.backward()\n",
    "                optimiser.step()\n",
    "                \n",
    "\n",
    "            model.eval()\n",
    "            \n",
    "            if i>eplim:\n",
    "                Loss = ia_loss\n",
    "            else:\n",
    "                Loss = ce_loss\n",
    "            # \"get_total_loss\": function defined in statml_tools.py to evaluate the network in batches (useful for large datasets)\n",
    "            train_loss.append(get_total_loss(model, Loss, X_train, y_train,w_train))\n",
    "            test_loss.append(get_total_loss(model, Loss, X_test, y_test,w_test))\n",
    "            t.set_postfix(train_loss=train_loss[-1], test_loss=test_loss[-1])\n",
    "            i+=1\n",
    "\n",
    "    print(\">> Training finished\")\n",
    "    model.eval()\n",
    "\n",
    "    return model, train_loss, test_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc = np.load(f'/vols/cms/hw423/Data/Week14/octest_{filecode}.npy')\n",
    "df = pd.DataFrame(oc)\n",
    "mi_series = pd.read_csv('/vols/cms/hw423/Week6/MI_balanced.csv')\n",
    "\n",
    "model_ia = Net(n_features=7, nodes=nodes, output_nodes=7)\n",
    "dfx=df\n",
    "label = pd.read_pickle('/vols/cms/hw423/Data/Week14/Label.pkl')\n",
    "dfy = pd.get_dummies(label)\n",
    "dfw = pd.read_pickle('/vols/cms/hw423/Data/Week14/weight.pkl')\n",
    "\n",
    "X_train, X_test, y_train, y_test, w_train, w_test = train_test_split(dfx, dfy,dfw, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [01:21<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "InfAwareLoss.forward() missing 1 required positional argument: 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_bce, train_loss_bce, test_loss_bce \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_network_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_ia\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_hp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_hp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/vols/cms/hw423/Data/Week14/df_InfA_RD_DPrmvd.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m col \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mgamma$\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mggH\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqqH\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWH\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZH\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mttH\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtH\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[114], line 51\u001b[0m, in \u001b[0;36mtrain_network_cross_entropy\u001b[0;34m(model, X_train, X_test, y_train, y_test, w_train, w_test, train_hp)\u001b[0m\n\u001b[1;32m     49\u001b[0m     Loss \u001b[38;5;241m=\u001b[39m ce_loss\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# \"get_total_loss\": function defined in statml_tools.py to evaluate the network in batches (useful for large datasets)\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[43mget_total_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw_train\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     52\u001b[0m test_loss\u001b[38;5;241m.\u001b[39mappend(get_total_loss(model, Loss, X_test, y_test,w_test))\n\u001b[1;32m     53\u001b[0m t\u001b[38;5;241m.\u001b[39mset_postfix(train_loss\u001b[38;5;241m=\u001b[39mtrain_loss[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], test_loss\u001b[38;5;241m=\u001b[39mtest_loss[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/vols/cms/hw423/Week14/NNfunctions.py:133\u001b[0m, in \u001b[0;36mget_total_loss\u001b[0;34m(model, loss, X, y, mean_over_batch)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_tensor, y_tensor \u001b[38;5;129;01min\u001b[39;00m get_batches([X, y], eval_batch_size):\n\u001b[1;32m    132\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(X_tensor)\n\u001b[0;32m--> 133\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(output))\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mean_over_batch:\n\u001b[1;32m    136\u001b[0m     mean_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(losses) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(losses)\n",
      "File \u001b[0;32m/vols/cms/hw423/env/envs/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: InfAwareLoss.forward() missing 1 required positional argument: 'weight'"
     ]
    }
   ],
   "source": [
    "model_bce, train_loss_bce, test_loss_bce = train_network_cross_entropy(model_ia, X_train, X_test, y_train, y_test,w_train,w_test, train_hp=train_hp)\n",
    "\n",
    "\n",
    "data = pd.read_parquet('/vols/cms/hw423/Data/Week14/df_InfA_RD_DPrmvd.parquet')\n",
    "col = ['$\\gamma\\gamma$','ggH','qqH','WH','ZH','ttH','tH']\n",
    "oc= model_bce(torch.tensor(data.to_numpy(),dtype=torch.float32))\n",
    "octest =pd.DataFrame(oc.detach(), columns = col, index = data.index)\n",
    "np.save(f'/vols/cms/hw423/Data/Week14/octest_{filecode}.npy', np.array(octest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          1.026104e-05\n",
       "1          2.839050e-06\n",
       "2          1.184555e-06\n",
       "3          2.860377e-06\n",
       "4          2.077187e-06\n",
       "               ...     \n",
       "2914418    9.387862e-11\n",
       "2914419    9.846524e-11\n",
       "2914420    8.756236e-11\n",
       "2914421    6.371580e-11\n",
       "2914422    8.610738e-11\n",
       "Name: weight, Length: 2914423, dtype: float32"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oc = np.load(f'/vols/cms/hw423/Data/Week14/octest_{filecode}.npy')\n",
    "oc = np.load(f'/vols/cms/hw423/Data/Week14/octest_InfA_RD_DPrmvd.npy')\n",
    "df = pd.DataFrame(oc)\n",
    "label = pd.read_pickle('/vols/cms/hw423/Data/Week14/Label.pkl')\n",
    "weight = pd.read_pickle('/vols/cms/hw423/Data/Week14/weight.pkl')\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>527151</th>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.728396</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.029366</td>\n",
       "      <td>0.241861</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440105</th>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.788625</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.023760</td>\n",
       "      <td>0.187225</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2237332</th>\n",
       "      <td>0.001539</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>0.011481</td>\n",
       "      <td>0.001539</td>\n",
       "      <td>0.005213</td>\n",
       "      <td>0.885481</td>\n",
       "      <td>0.089619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681188</th>\n",
       "      <td>0.006859</td>\n",
       "      <td>0.070842</td>\n",
       "      <td>0.192470</td>\n",
       "      <td>0.006859</td>\n",
       "      <td>0.036199</td>\n",
       "      <td>0.186599</td>\n",
       "      <td>0.500171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148646</th>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.554309</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.061152</td>\n",
       "      <td>0.382614</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692743</th>\n",
       "      <td>0.004447</td>\n",
       "      <td>0.025748</td>\n",
       "      <td>0.101407</td>\n",
       "      <td>0.004447</td>\n",
       "      <td>0.011072</td>\n",
       "      <td>0.586202</td>\n",
       "      <td>0.266676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2356330</th>\n",
       "      <td>0.001893</td>\n",
       "      <td>0.007540</td>\n",
       "      <td>0.013207</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>0.006946</td>\n",
       "      <td>0.815757</td>\n",
       "      <td>0.152765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2229084</th>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.126245</td>\n",
       "      <td>0.006038</td>\n",
       "      <td>0.339457</td>\n",
       "      <td>0.523407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2768307</th>\n",
       "      <td>0.005742</td>\n",
       "      <td>0.020084</td>\n",
       "      <td>0.071224</td>\n",
       "      <td>0.005742</td>\n",
       "      <td>0.041305</td>\n",
       "      <td>0.449327</td>\n",
       "      <td>0.406576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2219110</th>\n",
       "      <td>0.002223</td>\n",
       "      <td>0.010071</td>\n",
       "      <td>0.667608</td>\n",
       "      <td>0.002223</td>\n",
       "      <td>0.005035</td>\n",
       "      <td>0.004981</td>\n",
       "      <td>0.307860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2331538 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6\n",
       "527151   0.000094  0.728396  0.000094  0.029366  0.241861  0.000094  0.000094\n",
       "1440105  0.000091  0.788625  0.000118  0.023760  0.187225  0.000091  0.000091\n",
       "2237332  0.001539  0.005128  0.011481  0.001539  0.005213  0.885481  0.089619\n",
       "681188   0.006859  0.070842  0.192470  0.006859  0.036199  0.186599  0.500171\n",
       "1148646  0.000425  0.554309  0.000649  0.061152  0.382614  0.000425  0.000425\n",
       "...           ...       ...       ...       ...       ...       ...       ...\n",
       "1692743  0.004447  0.025748  0.101407  0.004447  0.011072  0.586202  0.266676\n",
       "2356330  0.001893  0.007540  0.013207  0.001893  0.006946  0.815757  0.152765\n",
       "2229084  0.001617  0.001617  0.001617  0.126245  0.006038  0.339457  0.523407\n",
       "2768307  0.005742  0.020084  0.071224  0.005742  0.041305  0.449327  0.406576\n",
       "2219110  0.002223  0.010071  0.667608  0.002223  0.005035  0.004981  0.307860\n",
       "\n",
       "[2331538 rows x 7 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
