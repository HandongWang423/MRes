xgb_Wd_sftmx_md10_x7x3_5e3_4
{'lr': 0.0001, 'batch_size': 10000, 'BS': 20000, 'N_epochs': 200, 'seed': 0}
>> Training...
  0%|          | 0/200 [00:00<?, ?it/s]/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  theta = torch.tensor(theta_init)
/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(out)
  0%|          | 0/200 [00:13<?, ?it/s, test_loss=21.2, train_loss=17.3]  0%|          | 1/200 [00:13<44:44, 13.49s/it, test_loss=21.2, train_loss=17.3]  0%|          | 1/200 [00:26<44:44, 13.49s/it, test_loss=18, train_loss=14.6]    1%|          | 2/200 [00:26<43:56, 13.31s/it, test_loss=18, train_loss=14.6]  1%|          | 2/200 [00:40<43:56, 13.31s/it, test_loss=15.5, train_loss=12.6]  2%|▏         | 3/200 [00:40<43:43, 13.32s/it, test_loss=15.5, train_loss=12.6]  2%|▏         | 3/200 [00:53<43:43, 13.32s/it, test_loss=11.5, train_loss=9.39]  2%|▏         | 4/200 [00:53<43:43, 13.39s/it, test_loss=11.5, train_loss=9.39]  2%|▏         | 4/200 [01:06<43:43, 13.39s/it, test_loss=5.89, train_loss=4.58]  2%|▎         | 5/200 [01:06<43:34, 13.41s/it, test_loss=5.89, train_loss=4.58]  2%|▎         | 5/200 [01:20<43:34, 13.41s/it, test_loss=3.93, train_loss=3.3]   3%|▎         | 6/200 [01:20<43:17, 13.39s/it, test_loss=3.93, train_loss=3.3]  3%|▎         | 6/200 [01:33<43:17, 13.39s/it, test_loss=4.28, train_loss=3.46]  4%|▎         | 7/200 [01:33<43:06, 13.40s/it, test_loss=4.28, train_loss=3.46]  4%|▎         | 7/200 [01:46<43:06, 13.40s/it, test_loss=3.77, train_loss=2.97]  4%|▍         | 8/200 [01:46<42:36, 13.32s/it, test_loss=3.77, train_loss=2.97]  4%|▍         | 8/200 [02:00<42:36, 13.32s/it, test_loss=3.82, train_loss=3.22]  4%|▍         | 9/200 [02:00<42:24, 13.32s/it, test_loss=3.82, train_loss=3.22]  4%|▍         | 9/200 [02:13<42:24, 13.32s/it, test_loss=3.55, train_loss=2.92]  5%|▌         | 10/200 [02:13<42:10, 13.32s/it, test_loss=3.55, train_loss=2.92]  5%|▌         | 10/200 [02:26<42:10, 13.32s/it, test_loss=3.72, train_loss=3.04]  6%|▌         | 11/200 [02:26<42:03, 13.35s/it, test_loss=3.72, train_loss=3.04]  6%|▌         | 11/200 [02:40<42:03, 13.35s/it, test_loss=3.7, train_loss=3.02]   6%|▌         | 12/200 [02:40<41:49, 13.35s/it, test_loss=3.7, train_loss=3.02]  6%|▌         | 12/200 [02:53<41:49, 13.35s/it, test_loss=3.64, train_loss=3.04]  6%|▋         | 13/200 [02:53<41:34, 13.34s/it, test_loss=3.64, train_loss=3.04]  6%|▋         | 13/200 [03:06<41:34, 13.34s/it, test_loss=3.62, train_loss=3.02]  7%|▋         | 14/200 [03:06<41:16, 13.32s/it, test_loss=3.62, train_loss=3.02]  7%|▋         | 14/200 [03:20<41:16, 13.32s/it, test_loss=3.54, train_loss=2.97]  8%|▊         | 15/200 [03:20<40:56, 13.28s/it, test_loss=3.54, train_loss=2.97]  8%|▊         | 15/200 [03:33<40:56, 13.28s/it, test_loss=3.72, train_loss=3.01]  8%|▊         | 16/200 [03:33<40:45, 13.29s/it, test_loss=3.72, train_loss=3.01]  8%|▊         | 16/200 [03:46<40:45, 13.29s/it, test_loss=3.89, train_loss=3.17]  8%|▊         | 17/200 [03:46<40:26, 13.26s/it, test_loss=3.89, train_loss=3.17]  8%|▊         | 17/200 [04:00<40:26, 13.26s/it, test_loss=3.84, train_loss=3.15]  9%|▉         | 18/200 [04:00<40:26, 13.33s/it, test_loss=3.84, train_loss=3.15]  9%|▉         | 18/200 [04:13<40:26, 13.33s/it, test_loss=3.68, train_loss=3]    10%|▉         | 19/200 [04:13<40:14, 13.34s/it, test_loss=3.68, train_loss=3] 10%|▉         | 19/200 [04:26<40:14, 13.34s/it, test_loss=3.56, train_loss=2.88] 10%|█         | 20/200 [04:26<39:58, 13.33s/it, test_loss=3.56, train_loss=2.88] 10%|█         | 20/200 [04:35<41:17, 13.76s/it, test_loss=3.56, train_loss=2.88]
Traceback (most recent call last):
  File "/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py", line 215, in <module>
    model, train_loss_ia, test_loss_ia = train_network(model_ia, X_train, X_test, y_train, y_test,w_train,w_test, train_hp=train_hp)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vols/cms/hw423/Week14/A_Post_trains/InferenceAware_Trainning.py", line 170, in train_network
    raise ValueError("Loss is NaN, terminating training")
ValueError: Loss is NaN, terminating training
